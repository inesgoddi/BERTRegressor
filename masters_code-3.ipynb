{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fO2nGhvXxkMO"
      },
      "outputs": [],
      "source": [
        "# Importations nécessaires\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import seaborn as sns\n",
        "import string\n",
        "import nltk\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Téléchargement des ressources NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Chargement des fichiers CSV\n",
        "train_df = pd.read_csv(\"train-m.csv\", on_bad_lines='skip')\n",
        "val_df = pd.read_csv(\"val-m.csv\", on_bad_lines='skip')\n",
        "test_df = pd.read_csv(\"test-m.csv\", on_bad_lines='skip')\n",
        "scores_df = pd.read_csv(\"test_scores-m.csv\", on_bad_lines='skip')  # facultatif\n",
        "\n",
        "# Load your BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
        "\n",
        "# Tokenize using BERT for consistent downstream usage\n",
        "df['bert_tokens'] = df['text'].apply(lambda x: tokenizer.tokenize(str(x)))\n",
        "df['bert_token_len'] = df['bert_tokens'].apply(len)\n",
        "\n",
        "# Class distribution\n",
        "class_counts = df['label'].value_counts().sort_index()\n",
        "print(\"Nombre de textes par niveau :\\n\", class_counts)\n",
        "\n",
        "# Average length per class\n",
        "avg_length = df.groupby('label')['bert_token_len'].mean()\n",
        "print(\"\\nLongueur moyenne (tokens BERT) par niveau :\\n\", avg_length)\n",
        "\n",
        "# Vocabulary size per class (using BERT tokens)\n",
        "def get_vocab(tokens_list):\n",
        "    vocab = set()\n",
        "    for tokens in tokens_list:\n",
        "        vocab.update(tokens)\n",
        "    return vocab\n",
        "\n",
        "vocab_by_class = {\n",
        "    label: get_vocab(df[df['label'] == label]['bert_tokens'])\n",
        "    for label in sorted(df['label'].unique())\n",
        "}\n",
        "for label, vocab_set in vocab_by_class.items():\n",
        "    print(f\"Vocabulaire BERT unique pour le niveau {label} : {len(vocab_set)} tokens\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset as TorchDataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Dataset\n",
        "class Dataset(TorchDataset):\n",
        "    def __init__(self, df, labels, max_length=512):\n",
        "        self.max_length = max_length\n",
        "        self.labels = []\n",
        "        self.texts = []\n",
        "\n",
        "        for idx, text in enumerate(df['text']):\n",
        "            if isinstance(text, str) and text.strip():\n",
        "                encoded = tokenizer.encode_plus(\n",
        "                    text,\n",
        "                    add_special_tokens=True,\n",
        "                    max_length=max_length,\n",
        "                    padding='max_length',\n",
        "                    truncation=True,\n",
        "                    return_tensors='pt'\n",
        "                )\n",
        "                self.texts.append(encoded)\n",
        "                self.labels.append(labels[str(df['label'].iloc[idx])])\n",
        "\n",
        "        if len(self.labels) != len(self.texts):\n",
        "            raise ValueError(\"Mismatch between number of texts and labels.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        return text['input_ids'].squeeze(0), text['attention_mask'].squeeze(0), label\n",
        "\n",
        "\n",
        "# Model\n",
        "class BertRegressor(nn.Module):\n",
        "    def __init__(self, dropout=0.2):\n",
        "        super(BertRegressor, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.regressor = nn.Linear(self.bert.config.hidden_size, 1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_state = outputs.hidden_states[-1]\n",
        "        cls_embeddings = last_hidden_state[:, 0, :]\n",
        "        cls_embeddings = self.dropout(cls_embeddings)\n",
        "        self.cls_embeddings = cls_embeddings  # store for later use\n",
        "        prediction = self.regressor(cls_embeddings)\n",
        "        return prediction.view(-1)  # Ensures output is always [batch_size]\n",
        "\n",
        "\n",
        "# Train function\n",
        "def train(model, train_df, train_labels, val_df, val_labels, learning_rate, epochs):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.MSELoss().to(device)\n",
        "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    train_dataset = Dataset(train_df, train_labels)\n",
        "    val_dataset = Dataset(val_df, val_labels)\n",
        "\n",
        "    def collate_fn(batch):\n",
        "        input_ids = pad_sequence([item[0] for item in batch], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "        attention_mask = pad_sequence([item[1] for item in batch], batch_first=True, padding_value=0)\n",
        "        labels = torch.tensor([item[2] for item in batch], dtype=torch.float)\n",
        "        return {'input_ids': input_ids, 'attention_mask': attention_mask}, labels\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    accumulation_steps = 4\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        cls_embeddings_with_text_train = []\n",
        "        cls_embeddings_with_text_val = []\n",
        "        model.train()\n",
        "        total_loss_train = 0\n",
        "\n",
        "        for i, (train_input, train_label) in enumerate(tqdm(train_loader)):\n",
        "            train_label = train_label.to(device)\n",
        "            input_ids = train_input['input_ids'].to(device)\n",
        "            attention_mask = train_input['attention_mask'].to(device)\n",
        "\n",
        "            output = model(input_ids, attention_mask)\n",
        "            loss = criterion(output, train_label)\n",
        "            total_loss_train += loss.item()\n",
        "\n",
        "            (loss / accumulation_steps).backward()\n",
        "            if (i + 1) % accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            cls_embeddings_with_text_train.extend(zip(\n",
        "                [tokenizer.decode(ids, skip_special_tokens=True) for ids in input_ids],\n",
        "                model.cls_embeddings.detach().cpu().numpy(),\n",
        "                train_label.cpu().numpy()\n",
        "            ))\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        total_loss_val = 0\n",
        "        with torch.no_grad():\n",
        "            for val_input, val_label in val_loader:\n",
        "                val_label = val_label.to(device)\n",
        "                input_ids = val_input['input_ids'].to(device)\n",
        "                attention_mask = val_input['attention_mask'].to(device)\n",
        "\n",
        "                output = model(input_ids, attention_mask)\n",
        "                loss = criterion(output, val_label)\n",
        "                total_loss_val += loss.item()\n",
        "\n",
        "                cls_embeddings_with_text_val.extend(zip(\n",
        "                    [tokenizer.decode(ids, skip_special_tokens=True) for ids in input_ids],\n",
        "                    model.cls_embeddings.detach().cpu().numpy(),\n",
        "                    val_label.cpu().numpy()\n",
        "                ))\n",
        "\n",
        "        print(f\"Epoch {epoch + 1} | Train Loss: {total_loss_train:.4f} | Val Loss: {total_loss_val:.4f}\")\n",
        "\n",
        "    return cls_embeddings_with_text_train, cls_embeddings_with_text_val\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XfwI17omyET3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "LR = 2e-05\n",
        "\n",
        "#Best Parameters: {'learning_rate': 2e-05, 'dropout': 0.1, 'epochs': 5, 'patience': 2}\n",
        "\n",
        "# Regression labels should be float\n",
        "labels = {'0': 0.0, '1': 1.0, '2': 2.0}\n",
        "\n",
        "# Initialize model for regression\n",
        "model = BertRegressor()\n",
        "\n",
        "# Train the model\n",
        "cls_embeddings_with_text_train, cls_embeddings_with_text_val = train(\n",
        "    model,\n",
        "    train_df, labels,\n",
        "    val_df, labels,\n",
        "    LR,\n",
        "    EPOCHS\n",
        ")\n"
      ],
      "metadata": {
        "id": "tFzJ7ySByVV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lifelines"
      ],
      "metadata": {
        "id": "_d4wXAAMzOhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from scipy.stats import pearsonr\n",
        "from lifelines.utils import concordance_index\n",
        "# Evaluation function\n",
        "def evaluate_regression(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Evaluate regression results using multiple metrics.\n",
        "\n",
        "    Parameters:\n",
        "    - y_true: List or array of true labels\n",
        "    - y_pred: List or array of predicted labels\n",
        "\n",
        "    Returns:\n",
        "    - Dictionary of evaluation metrics\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    metrics = {}\n",
        "    metrics['MSE'] = mean_squared_error(y_true, y_pred)\n",
        "    metrics['MAE'] = mean_absolute_error(y_true, y_pred)\n",
        "    metrics['R2'] = r2_score(y_true, y_pred)\n",
        "\n",
        "    # Pearson correlation (returns a tuple of correlation and p-value)\n",
        "    pearson_corr, _ = pearsonr(y_true, y_pred)\n",
        "    metrics['Pearson'] = pearson_corr\n",
        "\n",
        "    # Concordance Index (lifelines)\n",
        "    metrics['Concordance Index'] = concordance_index(y_true, y_pred)\n",
        "\n",
        "    print(\"Evaluation Results:\")\n",
        "    for k, v in metrics.items():\n",
        "        print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "        input_ids = pad_sequence([item[0] for item in batch], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "        attention_mask = pad_sequence([item[1] for item in batch], batch_first=True, padding_value=0)\n",
        "        labels = torch.tensor([item[2] for item in batch], dtype=torch.float)\n",
        "        return {'input_ids': input_ids, 'attention_mask': attention_mask}, labels\n",
        "\n",
        "# Create the test dataset and DataLoader\n",
        "test_dataset = Dataset(test_df, labels)  # Assuming 'labels' is the same for the test set\n",
        "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Function to get predictions for test dataset\n",
        "def get_predictions_on_test(model, test_loader, tokenizer):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    truths = []\n",
        "    cls_embeddings = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for test_input, test_label in test_loader:\n",
        "            input_ids = test_input['input_ids'].to(device)\n",
        "            attention_mask = test_input['attention_mask'].to(device)\n",
        "\n",
        "            output = model(input_ids, attention_mask)  # model.cls_embeddings is set here\n",
        "\n",
        "            preds.extend(output.cpu().numpy())\n",
        "            truths.extend(test_label.numpy())\n",
        "            cls_embeddings.append(model.cls_embeddings.cpu())  # collect CLS embeddings\n",
        "\n",
        "    cls_embeddings = torch.cat(cls_embeddings, dim=0).numpy()\n",
        "    return truths, preds, cls_embeddings\n",
        "\n",
        "\n",
        "# Get predictions on the test set\n",
        "truths_test, preds_test,cls_embeddings_test = get_predictions_on_test(model, test_loader, tokenizer)\n",
        "\n",
        "# Now evaluate the model on the test set\n",
        "evaluate_regression(truths_test, preds_test)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G6yB2ystzKwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a defaultdict to accumulate vectors and count for each label\n",
        "from collections import defaultdict\n",
        "\n",
        "aggregated = defaultdict(lambda: {'embedding': []})\n",
        "\n",
        "# Group by label, accumulate sum of vectors, and count the occurrences\n",
        "for doc_id,cls_embedding in enumerate(cls_embeddings_test):\n",
        "\n",
        "    aggregated[doc_id]['embedding'].append(cls_embedding)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UkHatH3BynJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensors_list_beginner =  [torch.tensor(t[1]) for t in cls_embeddings_with_text_train if t[2].item() == 0]  # Convert to tensors\n",
        "tensors_list_intermediate =  [torch.tensor(t[1]) for t in cls_embeddings_with_text_train if t[2].item() == 1]\n",
        "tensors_list_expert =  [torch.tensor(t[1]) for t in cls_embeddings_with_text_train if t[2].item() == 2]  # Convert to tensors\n",
        "\n",
        "\n",
        "mean_tensor_beginner = torch.mean(torch.stack(tensors_list_beginner), dim=0)\n",
        "mean_tensor_expert = torch.mean(torch.stack(tensors_list_expert), dim=0)\n",
        "mean_tensor_intermediate = torch.mean(torch.stack(tensors_list_intermediate), dim=0)"
      ],
      "metadata": {
        "id": "tZ1t3szmF5u-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for doc_id in aggregated.keys():\n",
        "    test_mean_cls_embedding = aggregated[doc_id]['embedding']\n",
        "\n",
        "    label = test_df['label'][doc_id ]\n",
        "\n",
        "    # True label\n",
        "    y_true.append(label)\n",
        "\n",
        "    # Normalize test embedding\n",
        "    cls_test_normalized = test_mean_cls_embedding / np.linalg.norm(test_mean_cls_embedding)\n",
        "\n",
        "    # Normalize class mean embeddings\n",
        "    cls_beginner_normalized = mean_tensor_beginner.cpu().detach().numpy() / np.linalg.norm(mean_tensor_beginner.cpu().detach().numpy())\n",
        "    cls_intermediate_normalized = mean_tensor_intermediate.cpu().detach().numpy() / np.linalg.norm(mean_tensor_intermediate.cpu().detach().numpy())\n",
        "    cls_expert_normalized = mean_tensor_expert.cpu().detach().numpy() / np.linalg.norm(mean_tensor_expert.cpu().detach().numpy())\n",
        "\n",
        "    # Calculate cosine similarities\n",
        "    similarity_beg = cosine_similarity(cls_test_normalized.reshape(1, -1), cls_beginner_normalized.reshape(1, -1))[0][0]\n",
        "    similarity_intermediate = cosine_similarity(cls_test_normalized.reshape(1, -1), cls_intermediate_normalized.reshape(1, -1))[0][0]\n",
        "    similarity_expert = cosine_similarity(cls_test_normalized.reshape(1, -1), cls_expert_normalized.reshape(1, -1))[0][0]\n",
        "\n",
        "    # Predict label\n",
        "    similarities = [similarity_beg, similarity_intermediate, similarity_expert]\n",
        "    predicted_label = np.argmax(similarities)  # 0 = beginner, 1 = intermediate, 2 = expert\n",
        "\n",
        "    y_pred.append(predicted_label)\n",
        "\n",
        "# Now calculate confusion matrix and classification report\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "class_report = classification_report(y_true, y_pred, target_names=['beginner', 'intermediate', 'expert'])\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "print(\"\\nClassification Report:\\n\", class_report)\n"
      ],
      "metadata": {
        "id": "HtTLsiV4ypnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# 1. Exact Match Accuracy\n",
        "acc = 0\n",
        "for i in range(len(truths_test)):\n",
        "    if int(truths_test[i]) == int(scores_df['value'][i]):\n",
        "        acc += 1\n",
        "    #print(int(truths_test[i]), test_scores[i])\n",
        "print(\"Exact Match Accuracy:\", acc / len(truths_test))\n",
        "\n",
        "# 2. Convert to NumPy arrays\n",
        "y_true = np.array(truths_test, dtype=float)\n",
        "y_pred = np.array(scores_df['value'], dtype=float)\n",
        "\n",
        "# 3. Mean Absolute Error\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n",
        "\n",
        "# 4. Root Mean Squared Error\n",
        "mse = mean_squared_error(y_true, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "\n",
        "# 5. R-squared\n",
        "r2 = r2_score(y_true, y_pred)\n",
        "print(\"R-squared (R²):\", r2)\n"
      ],
      "metadata": {
        "id": "g1hraePSywer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Optional: restrict background to first 50 samples for speed\n",
        "background = cls_embeddings_test  # (M, D)\n",
        "cls_embeddings = np.array(cls_embeddings_test)  # (N, D)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def predict_from_cls(X):\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
        "    with torch.no_grad():\n",
        "        return model.regressor(X_tensor).cpu().numpy()\n",
        "\n",
        "# Create explainer\n",
        "explainer = shap.Explainer(predict_from_cls, background, algorithm=\"permutation\", max_evals=1600)\n",
        "\n",
        "# Compute SHAP values for the full test set\n",
        "shap_values = explainer(cls_embeddings)\n",
        "\n",
        "# ----- GLOBAL AGGREGATION -----\n",
        "# Compute mean absolute SHAP value per dimension (global importance)\n",
        "mean_abs_shap = np.abs(shap_values.values).mean(axis=0)  # shape: (D,)\n",
        "\n",
        "# Sort and display top dimensions\n",
        "top_k = 10\n",
        "top_dims = np.argsort(mean_abs_shap)[::-1][:top_k]\n",
        "\n",
        "print(\"Top SHAP dimensions (global importance):\")\n",
        "for i in top_dims:\n",
        "    print(f\"Dimension {i}: mean(|SHAP|) = {mean_abs_shap[i]:.4f}\")\n",
        "\n",
        "# Optional: bar plot for top-k\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar([f\"dim_{i}\" for i in top_dims], mean_abs_shap[top_dims])\n",
        "plt.title(\"Top Feature Importances (mean |SHAP|)\")\n",
        "plt.ylabel(\"Mean |SHAP Value|\")\n",
        "plt.show()\n",
        "\n",
        "# Optional SHAP summary plot (violin/beeswarm style)\n",
        "shap.summary_plot(\n",
        "    shap_values,\n",
        "    cls_embeddings,\n",
        "    feature_names=[f\"dim_{i}\" for i in range(cls_embeddings.shape[1])]\n",
        ")\n"
      ],
      "metadata": {
        "id": "0e9T0yjsy2mT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "cls_embeddings_test_distribution = [\n",
        "    (embedding, label)\n",
        "    for embedding, label in zip( cls_embeddings_test, preds_test)\n",
        "]"
      ],
      "metadata": {
        "id": "uOHZimfLy50z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "def plot_embedding_distribution(cls_embeddings_test_distribution, dimension_idx):\n",
        "    \"\"\"\n",
        "    Plots distribution, label relationship, and statistics for a given CLS embedding dimension.\n",
        "\n",
        "    Args:\n",
        "        cls_embeddings_with_text: list of (text, embedding, label) tuples\n",
        "        dimension_idx: integer, the dimension to explore\n",
        "    \"\"\"\n",
        "    # Extract embeddings and labels\n",
        "    embeddings = np.array([item[0] for item in cls_embeddings_test_distribution])  # shape (N, D)\n",
        "    labels = np.array([item[1] for item in cls_embeddings_test_distribution])       # shape (N,)\n",
        "    #print(labels)\n",
        "    # Get the values of the selected dimension\n",
        "    dim_values = embeddings[:, dimension_idx]\n",
        "\n",
        "    print(f\"Exploring dimension {dimension_idx}\")\n",
        "    print(f\"Embedding dimension stats: mean={dim_values.mean():.4f}, std={dim_values.std():.4f}\")\n",
        "\n",
        "    # Pearson correlation between this dimension and labels\n",
        "    corr, p_value = pearsonr(dim_values, labels)\n",
        "    print(f\"Pearson correlation with label: {corr:.4f} (p={p_value:.4e})\")\n",
        "\n",
        "    # --- Plot 1: Histogram of the dimension values\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.hist(dim_values, bins=30, edgecolor='k')\n",
        "    plt.title(f\"Histogram of Dimension {dimension_idx}\")\n",
        "    plt.xlabel(f\"Value of dim_{dimension_idx}\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "\n",
        "    # --- Plot 2: Scatter plot of dimension value vs label\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.scatter(dim_values, labels, alpha=0.6)\n",
        "    plt.title(f\"dim_{dimension_idx} vs Label\")\n",
        "    plt.xlabel(f\"Value of dim_{dimension_idx}\")\n",
        "    plt.ylabel(\"Label\")\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "# Suppose you have already trained and obtained:\n",
        "# cls_embeddings_with_text_train = [(text, embedding, label), ...]\n",
        "\n",
        "# Assume most important dimension from SHAP is 42\n",
        "plot_embedding_distribution(cls_embeddings_test_distribution , dimension_idx=667)\n"
      ],
      "metadata": {
        "id": "q6YdRaZay8i2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# 1. Exact Match Accuracy\n",
        "acc = 0\n",
        "for i in range(len(truths_test)):\n",
        "    if int(truths_test[i]) == int(scores_df['value'][i]):\n",
        "        acc += 1\n",
        "    #print(int(truths_test[i]), test_scores[i])\n",
        "print(\"Exact Match Accuracy:\", acc / len(truths_test))\n",
        "\n",
        "# 2. Convert to NumPy arrays\n",
        "y_true = np.array(truths_test, dtype=float)\n",
        "y_pred = np.array(scores_df['value'], dtype=float)\n",
        "\n",
        "# 3. Mean Absolute Error\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n",
        "\n",
        "# 4. Root Mean Squared Error\n",
        "mse = mean_squared_error(y_true, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "\n",
        "# 5. R-squared\n",
        "r2 = r2_score(y_true, y_pred)\n",
        "print(\"R-squared (R²):\", r2)\n"
      ],
      "metadata": {
        "id": "ZJcSdV4CMzDZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}